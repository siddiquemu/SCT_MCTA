tracktor:
  name: Tracktor++
  # Subfolder name in output/tracker/
  module_name: MOT17
  desription:
  seed: 12345
  # frcnn or fpn
  network: fpn

  # frcnn
  # obj_detect_weights: output/frcnn/res101/mot_2017_train/180k/res101_faster_rcnn_iter_180000.pth
  # obj_detect_config: output/frcnn/res101/mot_2017_train/180k/sacred_config.yaml

  # fpn
  obj_detect_model_base: /media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d612/tracking_wo_bnw/output/faster_rcnn_fpn_training_mot_17/model_epoch_27.model

  #wild-track data
  obj_detect_wildtrack_model: /media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d612/tracking_wo_bnw/output/training_wild_track/model_epoch_90.model
  #obj_detect_wildtrack_model: /media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/training_wild_trackv2/model_epoch_100.model
  #obj_detect_wildtrack_model: /media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/faster_rcnn_fpn_training_mot_17/model_epoch_27.model

  #trained on clasp2

  #best person class
  #clasp1-clasp2
  #obj_detect_model: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/faster_rcnn_fpn_training_clasp_gt_panet/model_epoch_100.model'
  obj_detect_model: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d62/tracking_wo_bnw/output/semi_SL_kri_exp2_train_det/iter4/models/model_epoch_100.model'
  obj_detect_model_R34: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d62/tracking_wo_bnw/output/semi_SL_kri_exp2_train_det/iter0/models/model_epoch_80.model'
  
  #pvd-SSL-SL
  obj_detect_model_iter6: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d62/tracking_wo_bnw/output/self-supervise-pvd-det/C/iter6/models/model_epoch_100.model'
  obj_detect_model_iter4: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-pvd-det/C/iter4/models/model_epoch_100.model'
  obj_detect_model_iter8: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-pvd-det/C/iter8/models/model_epoch_100.model'
  obj_detect_model_iter9: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-pvd-det/C/iter9/models/model_epoch_100.model'
  obj_detect_model_tso_iter7: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-pvd-det-tso/C/iter7/models/model_epoch_100.model'
  obj_detect_model_tso_iter8: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-pvd-det-tso/C/iter8/models/model_epoch_100.model'
  obj_detect_model_tso_iter9: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-pvd-det-tso/C/iter9/models/model_epoch_100.model'
  obj_detect_model_tso_iter10: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-pvd-det-tso/C/iter10/models/model_epoch_100.model'
  #obj_detect_model: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/semi-supervise-logan-det/C1/iter0_C1/model_epoch_40.model'
  obj_detect_model_R101: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/faster_rcnn_fpn_training_clasp_gt_panet_new/R101_KRI_gt/model_epoch_100.model'
  #clasp1: person, bag
  obj_detect_model_R101_clasp1: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/faster_rcnn_fpn_training_clasp1/R101_stage2/model_epoch_200.model'
  obj_detect_model_R50_clasp1: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/faster_rcnn_fpn_training_clasp1/R50_stage2/model_epoch_100.model'
  #TODO: clasp1: person, bag

  #logan data
  #obj_detect_logan_model: /media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d6/tracking_wo_bnw/output/train_logan_data/model_epoch_90.model
  #obj_detect_logan_model: /media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/self-supervise-logan-det/C5/iter2_C5/model_epoch_10.model
  #obj_detect_logan_model: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/logan_gt_model/best_model/model_epoch_170.model'
  #obj_detect_logan_model: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/logan_gt_model/model_epoch_110.model'
  obj_detect_logan_model: '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/tracking_wo_bnw/output/semi-supervise-logan-det/C1/iter0_C1/model_epoch_40.model'
  #TODO: Change the box predictor model using PANet
  # obj_detect_config: output/fpn/res101/mot_2017_train/voc_init_iccv19/config.yaml
  # obj_detect_weights: output/fpn/res101/mot19_cvpr_train/v1/fpn_1_3.pth
  # obj_detect_config: output/fpn/res101/mot19_cvpr_train/v1/config.yaml
  #TODO: fine-tuned re-id siamese net using clasp2
  reid_weights: output/tracktor/reid/res50-mot17-batch_hard/ResNet_iter_25245.pth
  reid_config: output/tracktor/reid/res50-mot17-batch_hard/sacred_config.yaml

  #reid_weights: output/tracktor/reid/train_gt/ResNet_iter_25020.pth
  #reid_config: output/tracktor/reid/sacred_config.yaml

  #reid_weights: output/tracktor/reid/train_gt_pvd/ResNet_iter_18800.pth
  #reid_config: output/tracktor/reid/sacred_config.yaml
  #for MCTA
  #cams: [2,9]
  interpolate: False
  # compile video with: `ffmpeg -f image2 -framerate 15 -i %06d.jpg -vcodec libx264 -y movie.mp4 -vf scale=320:-1`
  write_images: True
  # dataset (look into tracker/datasets/factory.py)
  #dataset: mot17_train_FRCNN17
  dataset: CLASP_train
  # [start percentage, end percentage], e.g., [0.0, 0.5] for train and [0.75, 1.0] for val split.
  frame_split: [0.0, 1.0]

  tracker:
    # FRCNN score threshold for detections
    detection_person_thresh: 0.8 #0.5,clasp: 0.7 wild-track: 0.6, clasp1:0.6
    # FRCNN score threshold for keeping the track alive
    regression_person_thresh: 0.7 #0.5, clasp:0.7 wild-track: 0.6
    # NMS threshold for detection
    detection_nms_thresh: 0.3 # wild-track: 0.4, clasp: 0.3
    # NMS theshold while tracking
    regression_nms_thresh: 0.6 #0.6
    # motion model settings
    motion_model:
      enabled: False #False
      # average velocity over last n_steps steps
      n_steps: 30 #10
      # if true, only model the movement of the bounding box center. If false, width and height are also modeled.
      center_only: True
    # DPM or DPM_RAW or 0, raw includes the unfiltered (no nms) versions of the provided detections,
    # 0 tells the tracker to use private detections (Faster R-CNN)
    public_detections: False
    # How much last appearance features are to keep
    max_features_num: 20 #clasp-10, wild-20
    # Do camera motion compensation
    do_align: False #True
    # Which warp mode to use (cv2.MOTION_EUCLIDEAN, cv2.MOTION_AFFINE, cv2.MOTION_HOMOGRAPHY ...)
    warp_mode: cv2.MOTION_EUCLIDEAN
    # maximal number of iterations (original 50)
    number_of_iterations: 100
    # Threshold increment between two iterations (original 0.001)
    termination_eps: 0.00001
    # Use siamese network to do reid
    do_reid: True
    # How much timesteps dead tracks are kept and cosidered for reid
    inactive_patience: 40 #clasp-40, 120-30fps # 1 Sec: 10 or 30
    # How similar do image and old track need to be to be considered the same person
    reid_sim_threshold: 2.0 #2.0 #2.0 #cosine appearance similarity distance
    # How much IoU do track and image need to be considered for matching
    reid_iou_threshold: 0.2 #0.2 #clasp:0.3 cam4:0.4
