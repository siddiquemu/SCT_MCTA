# from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import distutils.util
import os
import sys
import pprint
import subprocess
from collections import defaultdict
from six.moves import xrange

# Use a non-interactive backend
# import matplotlib
# matplotlib.use('Agg')

import numpy as np
import cv2
import time
import torch
import imutils
import pycocotools.mask as mask_util
from skimage import measure
import imageio
import glob
from pathlib import Path

# import imgaug.augmenters as iaa # TODO: to extract polygon section from image: train DAHE appearance model??
# import imgaug as ia
import torch.nn as nn
from torch.autograd import Variable
import _init_paths
import nn as mynn

from core.config import cfg, cfg_from_file, cfg_from_list, assert_and_infer_cfg
from core.test import im_detect_all, im_detect_regress, regressed_box2mask
from modeling.model_builder import Generalized_RCNN
import datasets.dummy_datasets as datasets
import utils.misc as misc_utils
import utils.net as net_utils
import utils.vis as vis_utils
from utils.process_box_mask import get_box_mask
from utils.detectron_weight_helper import load_detectron_weight
from utils.timer import Timer
from clasp2coco import define_dataset_dictionary, Write_To_Json, Write_ImagesInfo, Write_AnnotationInfo
from get_cluster_mode import Cluster_Mode
import random
import copy
import pandas as pd
import matplotlib.pyplot as plt
import pdb

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
   sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative
sys.path.append(str(FILE.parent.parent / "lib"))

# OpenCL may be enabled by default in OpenCV3; disable it because it's not
# thread safe and causes unwanted GPU memory allocations.
cv2.ocl.setUseOpenCL(False)

def convert_from_cls_format(cls_boxes, cls_segms=None, coarse_masks=None, cls_keyps=None):
    """Convert from the class boxes/segms/keyps format generated by the testing
    code.
    """
    box_list = [b for b in cls_boxes if len(b) > 0]
    if len(box_list) > 0:
        boxes = np.concatenate(box_list)
    else:
        boxes = None
    if cls_segms is not None:
        segms = [s for slist in cls_segms for s in slist]
    else:
        segms = None
    if cls_segms is not None:
        segms_coarse = [b for b in coarse_masks if len(b) > 0]
        coarse_masks_all = np.concatenate(segms_coarse)
    else:
        coarse_masks_all = None
    if cls_keyps is not None:
        keyps = [k for klist in cls_keyps for k in klist]
    else:
        keyps = None
    classes = []
    for j in range(len(cls_boxes)):
        classes += [j] * len(cls_boxes[j])
    return boxes, segms, coarse_masks_all, keyps, classes


def format_dets(cls_boxes, fr=None, dataset=None, class_list=None, thresh=0.5, class_id=None, angle=None):
    if isinstance(cls_boxes, list):
        # boxes, _,_, _, classes = convert_from_cls_format(cls_boxes)
        boxes = cls_boxes[class_id]
        classes = [class_id] * len(boxes)

    if boxes is None or boxes.shape[0] == 0 or max(boxes[:, 4]) < thresh:
        return

    # Display in largest to smallest order to reduce occlusion
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    sorted_inds = np.argsort(-areas)

    dets = []
    objP = 1
    for i in sorted_inds:
        bbox = boxes[i, :4]
        score = boxes[i, -1]
        w = bbox[2] - bbox[0] + 1
        h = bbox[3] - bbox[1] + 1
        w = np.maximum(w, 1)
        h = np.maximum(h, 1)
        # not necessary to use tis condition if model is finetuned dor PAX
        if classes[i] in class_list:
            box_remapd = [fr, objP, bbox[0], bbox[1], w, h, score, classes[i], angle]
            objP += 1

            dets.append(box_remapd)
            # print(dataset.classes[classes[i]], score)

    return np.array(dets)


def regress_dets(maskRCNN, proposals=None, im=None, im_name=None, fr=None, pred_score=None, dataset=None,
                 class_list=None, timers=None, output_dir=None, vis=False, img_fmt='jpg', class_id=None):
    # TODO: use all remapped dets as proposals to select the best candidate (filter partial dets)
    # TODO: convert all dets into torch variable
    # How to format proposals variable for multi-claass
    dets_proposals = copy.deepcopy(proposals[:, 2:6])
    dets_proposals[:, 2:4] = dets_proposals[:, 0:2] + dets_proposals[:, 2:4]
    # TODO: apply det_thr on the regressed augmented detections to reduce noises
    cls_boxes = im_detect_regress(maskRCNN, im,
                                  box_proposals=dets_proposals,
                                  timers=timers,
                                  test_aug=0,
                                  soft_nms=0,
                                  score_thr=0)

    det_size = sum(len(cls_boxes[cl]) for cl in class_list)
    # assert len(class_list)==1, 'regression for augmented dets only workd for single class model.. mulit-class: full input proposals are used for all classes'
    assert det_size // len(class_list) == dets_proposals.shape[0], 'proposals size {}, prediction size {}'.format(
        det_size, dets_proposals)

    if det_size > 0:
        dets = format_dets(cls_boxes, fr=fr, dataset=dataset, class_list=class_list,
                           thresh=0, class_id=class_id, angle=0)
    det_class = np.array(dets)
    assert len(det_class) == len(proposals)
    return det_class


def init_fig(im):
    im = im[:, :, ::-1]
    fig = plt.figure(frameon=False)
    fig.set_size_inches(im.shape[1] / 200, im.shape[0] / 200)
    ax = plt.Axes(fig, [0., 0., 1., 1.])
    ax.axis('off')
    fig.add_axes(ax)
    ax.imshow(im)
    return ax, fig


def get_masks(maskRCNN, detPB, im=None, vis_img=None, im_name=None, fr=None, pred_score=None, dataset=None,
              class_list=None, timers=None, vis_dir=None, vis=False, img_fmt='jpg', angle=None,
              class_id=None):
    # use this function only to refine the detections to use as annos
    # TODO: How to format proposals variable for multi-claass??
    dets_proposals = copy.deepcopy(detPB[:, 2:6])
    dets_proposals[:, 2:4] = dets_proposals[:, 0:2] + dets_proposals[:, 2:4]
    cls_boxes, cls_segms = regressed_box2mask(maskRCNN, im,
                                              box_proposals=dets_proposals,
                                              timers=timers,
                                              test_aug=0,
                                              soft_nms=0,
                                              score_thr=0)
    det_size = sum(len(cls_boxes[cl]) for cl in class_list)
    assert len(cls_boxes[class_id]) == len(detPB)
    # assert len(class_list)==1, 'regression for augmented dets only workd for single class model.. mulit-class: full input proposals are used for all classes'
    assert det_size // len(class_list) == dets_proposals.shape[0], 'proposals size {}, prediction size {}'.format(
        det_size, dets_proposals)
    if fr % 1 == 0 and vis and det_size > 0:
        vis_img = vis_utils.vis_clasp_annos(
            fr,
            angle,
            vis_img,  # BGR -> RGB for visualization
            im_name,
            vis_dir,
            cls_boxes,
            segms=cls_segms,
            dataset=dataset,
            class_list=[class_id],
            box_alpha=1,
            show_class=True,
            thresh=pred_score,
            kp_thresh=2,
            ext=img_fmt,
            show_mask=1
        )

    if det_size > 0:
        det_class, mask_class = get_box_mask(fr, cls_boxes, cls_segms, thresh=0,
                                             angle=angle, class_list=[class_id], dataset=dataset)
    else:
        mask_class = []
    det_class = np.array(det_class)
    assert len(det_class) == len(mask_class) == len(detPB)
    return det_class, mask_class, vis_img


def rotated_boxes(masks, img_rot=None, angle=None):
    # rle to binary mask
    # rotated boxes are axis aligned
    rot_boxes = []
    for mask in masks:
        mask_org = mask_util.decode(mask)
        # binary_mask = np.array(mask_image>0.5, dtype=np.uint8) # convert scoremap into binary mask
        # mask from segm_results
        mask_org[np.where(mask_org > 0)] = 255

        mask_rot = imutils.rotate_bound(mask_org, angle)  # mask_image scaled to rotated image size
        assert mask_rot.shape == img_rot.shape[0:2]
        # assert img_org.shape==im.shape[0:2], 'remapped mask image: {}, original image: {}'.format(img_org.shape, im.shape[0:2])
        rot_boxes.append(np.array(cv2.boundingRect(mask_rot)))
    return np.array(rot_boxes)


def get_split(GT, train_split=0.8, cam=None):
    """

    :param GT:
    :param train_splt:
    :param cam:
    :return:
    """
    GTp = [gt for gt in GT if gt[9] == 1]
    GTb = [gt for gt in GT if gt[9] == 2]
    gt_frameset = np.unique(GT[:, 0].astype('int'))
    gt_len = len(gt_frameset)
    print('full set {}: Nfr {}, person {} bag {}'.format(cam, gt_len, len(GTp), len(GTb)))

    # random split: keep split similar for training and testing forever
    random.seed(42)
    train_subset = random.sample(list(gt_frameset), int(gt_len * train_split))
    print('random sample {}'.format(train_subset[2]))
    # print(subset)
    train_GTp = [gt for gt in GT if gt[0] in train_subset and gt[9] == 1]
    train_GTb = [gt for gt in GT if gt[0] in train_subset and gt[9] == 2]
    print('train split {}: Nfr {}, person {} bag {}'.format(cam, len(train_subset), len(train_GTp), len(train_GTb)))

    test_subset = np.array([t for t in gt_frameset if t not in train_subset])
    test_GTp = [gt for gt in GT if gt[0] not in train_subset and gt[9] == 1]
    test_GTb = [gt for gt in GT if gt[0] not in train_subset and gt[9] == 2]
    print(
        'test split {}: Nfr {}, person {} bag {}'.format(cam, len(test_subset), len(test_GTp), len(test_GTb)))
    print('-------------------------------------------------')
    return train_subset, test_subset, train_GTp, train_GTb

def get_gts(database, image_dir, imglist=None):
    #open gt files
    if database in ['clasp1','clasp2']:
        GT = np.loadtxt(os.path.join(image_dir.split('img1')[0], 'gt/gt.txt'), delimiter=',')
        #train_set, test_set = get_split(GT, train_split=0.8, cam=cam)
        cam_folder = image_dir.split('/img1')[0].split('/')[-1]
        test_frames = pd.read_csv(os.path.join(image_dir.split('/img1')[0],'test_frames/{}.csv'.format(cam_folder)))
        test_set = test_frames.values.squeeze()
        print('cam {}, total test frames: {}'.format(cam_folder, len(test_set)))

        train_frames = pd.read_csv(os.path.join(image_dir.split('/img1')[0],'train_frames/{}.csv'.format(cam_folder)))
        train_set = train_frames.values.squeeze()
        print(f'cam {cam_folder}, total train frames: {len(train_set)}')

    else:
        test_set = [int(os.path.basename(im_name).split('.')[0]) for im_name in imglist]
    return train_set, test_set

def main(folders, angleSet, dataset_clasp, output_dir, isTrain, imgResultDir, database='clasp1', soft_nms=False,
         class_list=None, data_type='gt', save_data=False, saveAugResult=False,
         cluster_score_thr=[0, 0], det_thr=0.7, all_scores=None, regress_cluster=None, vis_annos=False):
    """main function"""
    # configure detector
    if not torch.cuda.is_available():
        sys.exit("Need a CUDA device to run the code.")
    torch.cuda.set_device(0)
    torch.set_num_threads(1)
    # args = parse_args()
    print('Called with args:')
    # print(args)

    if data == 'coco':
        dataset = datasets.get_coco_dataset()
        cfg.MODEL.NUM_CLASSES = len(dataset.classes)
    elif data == 'clasp2020':
        dataset = datasets.get_clasp_dataset()
        cfg.MODEL.NUM_CLASSES = len(dataset.classes)
    elif data == 'clasp1_2021':
        dataset = datasets.get_clasp1_dataset()
        cfg.MODEL.NUM_CLASSES = len(dataset.classes)
    elif data == "keypoints_coco":
        dataset = datasets.get_coco_dataset()
        cfg.MODEL.NUM_CLASSES = 2
    else:
        raise ValueError('Unexpected dataset name: {}'.format(data))

    print('load cfg from file: {}'.format(cfg_file))
    cfg_from_file(cfg_file)

    # set NMS
    cfg['TEST']['NMS'] = 0.3
    cfg['TEST']['SCORE_THRESH'] = det_thr

    if set_cfgs is not None:
        cfg_from_list(set_cfgs)
    assert bool(load_ckpt) ^ bool(load_detectron), \
        'Exactly one of --load_ckpt and --load_detectron should be specified.'
    cfg.MODEL.LOAD_IMAGENET_PRETRAINED_WEIGHTS = False  # Don't need to load imagenet pretrained weights
    assert_and_infer_cfg()
    maskRCNN = Generalized_RCNN()
    if cuda:
        maskRCNN.cuda()

    load_name = load_ckpt
    print("loading checkpoint %s" % (load_name))
    checkpoint = torch.load(load_name, map_location=lambda storage, loc: storage)
    if load_ckpt_frcnn:
        checkpoint_frcnn = torch.load(load_ckpt_frcnn, map_location=lambda storage, loc: storage)
        checkpoint_frcnn = checkpoint_frcnn['model']
    else:
        checkpoint_frcnn = None
    net_utils.load_ckpt(maskRCNN, checkpoint['model'], ckpt_frcnn=checkpoint_frcnn,
                        isTrain=False, FreezeResnetConv=False)

    if load_detectron:
        print("loading detectron weights %s" % load_detectron)
        load_detectron_weight(maskRCNN, load_detectron)

    maskRCNN = mynn.DataParallel(maskRCNN, cpu_keywords=['im_info', 'roidb'],
                                 minibatch=True, device_ids=[0])  # only support single GPU
    maskRCNN.eval()

    # save all images in one folder
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # read imgaes from multi-camera clasp2 data folders
    # cam_list = [folders[0],folders[2],folders[4],folders[5],folders[6]]
    fr = 1
    annIDcount = 1
    folders = sorted(folders)
    for cam_path in folders:
        # get training set
        image_dir = os.path.join(cam_path, 'img1')
        box_gt = np.loadtxt(os.path.join(cam_path, 'gt/gt.txt'), delimiter=',')
        #train_set, _, _, train_set_bag = get_split(box_gt, train_split=0.8, cam=cam_path.split('/')[-1])

        #TODO: verify the train set
        train_set, test_set = get_gts(database, cam_path)

        # camera annos dir
        vis_annos_dir = os.path.join(imgResultDir, 'test_results', cam_path.split('/')[-1])
        if not os.path.exists(vis_annos_dir):
            os.makedirs(vis_annos_dir)
        else:
            delete_all(vis_annos_dir, fmt='jpg')


        # create and clean path for visualization
        result_path = os.path.join(imgResultDir, cam_path.split('/')[-1])
        if not os.path.exists(result_path):
            os.makedirs(result_path)
        else:
            delete_all(result_path, fmt='jpg')


        # check image path
        assert image_dir or images
        assert bool(image_dir) ^ bool(images)
        if image_dir:
            imglist = sorted(glob.glob(os.path.join(image_dir, '*')))
            # imglist = misc_utils.get_imagelist_from_dir(image_dir)
        else:
            imglist = images
        num_images = len(imglist)

        # loop over all the annotated image
        for i, im_name in enumerate(imglist):
            fr_num = float(os.path.basename(im_name).split('.')[0])

            # search training set frames for augmented detections
            if fr_num in train_set and (fr_num) % 1 == 0:
                im = cv2.imread(im_name)

                blankImg = im
                rot_imgs_dict = {}
                detPB = []
                maskPB_rot = []
                detPB_rot = []
                fr_box_gt = box_gt[box_gt[:,0]==fr_num]
                for angle in angleSet:
                    if angle > 0:
                        print('Image: {}, Cam: {}, Rotated by: {},'.format(os.path.basename(im_name),
                                                                           im_name.split('/')[-3], angle))
                        imgrot = imutils.rotate_bound(blankImg, angle)
                    else:
                        imgrot = blankImg
                    rot_imgs_dict[angle] = copy.deepcopy(imgrot)

                    if len(fr_box_gt) > 0 and save_data:
                        fr_det = fr_box_gt

                        # save image info
                        imgrot = rot_imgs_dict[0]
                        imgIdnew = 10000 * int('%06d' % fr) + angle
                        imgname = '{:08d}.png'.format(imgIdnew)
                        img_write_path = output_dir + '/' + imgname

                        if not os.path.exists(img_write_path):
                            dataset_clasp = Write_ImagesInfo(imgrot, imgname, int(imgIdnew), dataset_clasp)
                            print(f'Writing image to {img_write_path}')
                            cv2.imwrite(img_write_path, imgrot)

                        dataset_clasp = Write_ImagesInfo(imgrot, imgname, int(imgIdnew), dataset_clasp)

                        # save det info
                        print('{} boxes annoted in {}'.format(len(fr_det), imgname))
                        for ib, box in enumerate(fr_det):
                            # TODO: no score threshold or cluster score instead of detection score: since only consider the cluster modes
                            # if (box[6]>=0.4 and box[7]==1) or (box[6]>=0.2 and box[7]!=1):
                            bboxfinal = [round(x, 2) for x in box[2:6]]

                            # [fr, i, bbox[0], bbox[1], w, h, score, classes[i]]
                            catID = int(box[9])
                            area = box[4] * box[5]

                            annID = 1000 * int('%06d' % (annIDcount)) + angle
                            annIDcount += 1
                            # box = mask_util.toBbox(mask)
                            # convert rle mask into polygon mask
                            # TODO: try to use rle format in coco annotation format
                            segmPolys = []  # mask['counts'].decode("utf-8") #[]

                            assert int(imgIdnew) == int(os.path.basename(img_write_path).split('.')[0])
                            # save annotation infor for each image
                            dataset_clasp = Write_AnnotationInfo(bboxfinal, segmPolys, int(imgIdnew),
                                                                 int(annID), catID, int(area), dataset_clasp)

                    fr += 1
    return dataset_clasp, all_scores


def delete_all(demo_path, fmt='png'):
    filelist = glob.glob(os.path.join(demo_path, '*.' + fmt))
    if len(filelist) > 0:
        for f in filelist:
            os.remove(f)


if __name__ == '__main__':
    database  = 'clasp1'
    data_type = 'clasp1_gt'  # 'clasp2_augMS'  # 'gt' #'test_aug
    storage = '/media/abubakar/PhD_Backup'
    isTrain = False
    # required inputs
    data = f'{database}_2021' #"coco"
    model_path = os.path.join(storage, 'models/clasp1')
    cfg_file = os.path.join(ROOT, 'configs/panet/e2e_panet_R-50-FPN_2x_mask.yaml')

    if data == 'coco':
        load_ckpt = os.path.join(storage, "/models/panet/panet_mask_step179999.pth")
        class_list = [1, 27, 25, 29]
    else:
        load_ckpt = os.path.join(model_path, 'modified_loss_semi/100_percent/iter7/ckpt/model_step19999.pth') # 'e2e_panet_R-50-FPN_2x_mask/panet_box_mask_tuned3k/ckpt/model_step9999.pth'
        class_list = [1, 2]

    load_ckpt_frcnn = None  # model_path+'e2e_panet_R-50-FPN_1x_det/panet_box_tuned/ckpt/model_step25524.pth'
    images = False
    set_cfgs = None
    load_detectron = None
    cuda = True
    merge_pdfs = False

    vis_annos = 0
    regress_cluster = 0
    save_data = 1
    test_aug = 0
    soft_nms = 1
    exp = 8
    # recommended: pax: 0.4, bag: 0.2
    # higher cluster score decrease the possibility of negative examples from the augmented dets...
    cluster_score_thr = [0.65, 0.5]  # 4-5:[0.6, 0.45]#3:[0.55, 0.4] #2:[0.5, 0.35]#clasp2: 0-1:[0.4, 0.25]
    det_thr = 0.5  # clasp2:7:0.5, 2-6:0.6 0-1:0.5

    if data_type == 'clasp1_gt':
        benchmark = os.path.join(storage, 'data/CLASP1')
        imgResultDir = os.path.join(benchmark, 'train_gt_det')
        savefilename = os.path.join(imgResultDir, 'clasp1_gt_det.json')
        SaveImgDir = os.path.join(imgResultDir, 'img1')

    # clear path files
    if os.path.exists(imgResultDir):
        delete_all(imgResultDir, fmt='png')
    else:
        os.makedirs(imgResultDir)
    if test_aug:
        # angleSet =[0, 12, 84, 90, 180, 186, 264, 270, 348, 354]
        angleSet = [0, 6, 12, 78, 84, 90, 96, 102, 168, 174, 180, 186, 192, 258, 264, 270, 276, 342, 348, 354]
        saveAugResult = 1

    else:
        angleSet = [0]
        saveAugResult = 0
    dataset_clasp = define_dataset_dictionary()

    benchmark_path = os.path.join(storage, 'data/CLASP1/train_gt')
    folders = glob.glob(benchmark_path + '*')
    folders.sort(key=lambda f: int(''.join(filter(str.isdigit, str(f)))))

    dataset_clasp, all_scores = main(folders, angleSet, dataset_clasp, SaveImgDir, isTrain, imgResultDir,
                                     database, soft_nms, class_list, data_type, save_data, saveAugResult,
                                     cluster_score_thr, det_thr, regress_cluster, vis_annos)
    if save_data:
        Write_To_Json(savefilename, dataset_clasp)

