#from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import distutils.util
import os
import sys
import pprint
import subprocess
from collections import defaultdict
from six.moves import xrange

# Use a non-interactive backend
#import matplotlib
#matplotlib.use('Agg')

import numpy as np
import cv2
import time
import torch
import imutils
import pycocotools.mask as mask_util
from skimage import measure
import imageio
import glob
#import imgaug.augmenters as iaa # TODO: to extract polygon section from image: train DAHE appearance model??
#import imgaug as ia
import torch.nn as nn
from torch.autograd import Variable
import _init_paths
import nn as mynn
from core.config import cfg, cfg_from_file, cfg_from_list, assert_and_infer_cfg
from core.test import im_detect_all, im_detect_regress, regressed_box2mask
from modeling.model_builder import Generalized_RCNN
import datasets.dummy_datasets as datasets
import utils.misc as misc_utils
import utils.net as net_utils
import utils.vis as vis_utils
from utils.process_box_mask import get_box_mask
from utils.detectron_weight_helper import load_detectron_weight
from utils.timer import Timer
from clasp2coco import define_dataset_dictionary, Write_To_Json,\
    Write_ImagesInfo, Write_AnnotationInfo, load_clasp_json, get_frame_anns
from get_cluster_mode import Cluster_Mode
import random
import copy
import pandas as pd
import matplotlib.pyplot as plt
import pdb
# OpenCL may be enabled by default in OpenCV3; disable it because it's not
# thread safe and causes unwanted GPU memory allocations.
cv2.ocl.setUseOpenCL(False)

def convert_from_cls_format(cls_boxes, cls_segms=None,coarse_masks=None, cls_keyps=None):
    """Convert from the class boxes/segms/keyps format generated by the testing
    code.
    """
    box_list = [b for b in cls_boxes if len(b) > 0]
    if len(box_list) > 0:
        boxes = np.concatenate(box_list)
    else:
        boxes = None
    if cls_segms is not None:
        segms = [s for slist in cls_segms for s in slist]
    else:
        segms = None
    if cls_segms is not None:
       segms_coarse = [b for b in coarse_masks if len(b) > 0]
       coarse_masks_all = np.concatenate(segms_coarse)
    else:
        coarse_masks_all = None
    if cls_keyps is not None:
        keyps = [k for klist in cls_keyps for k in klist]
    else:
        keyps = None
    classes = []
    for j in range(len(cls_boxes)):
        classes += [j] * len(cls_boxes[j])
    return boxes, segms, coarse_masks_all, keyps, classes

def format_dets( cls_boxes, fr=None, dataset=None, class_list=None, thresh=0, class_id=None, angle=None):

    if isinstance(cls_boxes, list):
        #boxes, _,_, _, classes = convert_from_cls_format(cls_boxes)
        boxes = cls_boxes[class_id]
        classes = [class_id] * len(boxes)

    if boxes is None or boxes.shape[0] == 0 or max(boxes[:, 4]) < thresh:
        return

    # Display in largest to smallest order to reduce occlusion
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    sorted_inds = np.argsort(-areas)

    dets = []
    objP=1
    for i in sorted_inds:
        bbox = boxes[i, :4]
        score = boxes[i, -1]
        w = bbox[2] - bbox[0] + 1
        h = bbox[3] - bbox[1] + 1
        w = np.maximum(w, 1)
        h = np.maximum(h, 1)
        # not necessary to use tis condition if model is finetuned dor PAX
        if classes[i] in class_list:
            box_remapd = [fr, objP, bbox[0], bbox[1], w, h, score, classes[i], angle]
            objP+=1

            dets.append(box_remapd)
            #print(dataset.classes[classes[i]], score)

    return np.array(dets)

def regress_dets(maskRCNN, proposals=None, im=None, im_name=None, fr=None, pred_score=None, dataset=None,
                 class_list=None, timers=None, isNMS=0, nms_thr=0.5, output_dir=None, vis=False, img_fmt='jpg', class_id=None):

    # How to format proposals variable for multi-claass: currently single class
    dets_proposals = copy.deepcopy(proposals[:, 2:6])
    dets_proposals[:, 2:4] = dets_proposals[:, 0:2] + dets_proposals[:, 2:4]
    # TODO: apply det_thr on the regressed augmented detections to reduce noises
    # TODO: for iter0, the single class det_proposals are used in 81 class predictor where input frame is unrotated version
    # !!!! For iter0, the augmented proposals may not be classified correctly among 81 classes
    # !!!! Avoid regression for iter0 to skip the wrong classification issue
    # !!!! solve the impact of unoriented frame uncertainty on the augmented proposals:
    # <<<<do not update the prediction score of the augmented proposals after regression>>>>
    cls_boxes = im_detect_regress(maskRCNN, im,
                                    box_proposals=dets_proposals,
                                    timers=timers,
                                    test_aug=0,
                                    soft_nms=isNMS,
                                    nms_thr=nms_thr,
                                    score_thr=pred_score)

    #TODO: proposals might be classified as a wrong class: augmented bag proposals are regressed as person class (currently ignored)
    det_size = len(cls_boxes[class_id])
    #assert len(class_list)==1, 'regression for augmented dets only workd for single class model.. mulit-class: full input proposals are used for all classes'
    #assert det_size//len(class_list)==dets_proposals.shape[0], 'proposals size {}, prediction size {}'.format(det_size, dets_proposals)

    if det_size > 0:
        dets = format_dets(cls_boxes, fr=fr, dataset=dataset, class_list=class_list,
                           thresh=0, class_id=class_id, angle=0)
        det_class = np.array(dets)
    else:
        det_class=np.empty(0)
    #assert len(det_class)==len(proposals)
    return det_class

def init_fig(im):
    im = im[:, :, ::-1]
    fig = plt.figure(frameon=False)
    fig.set_size_inches(im.shape[1] / 200, im.shape[0] / 200)
    ax = plt.Axes(fig, [0., 0., 1., 1.])
    ax.axis('off')
    fig.add_axes(ax)
    ax.imshow(im)
    return ax, fig

def get_masks(maskRCNN, detPB, im=None, vis_img=None, im_name=None, fr=None, pred_score=None, dataset=None,
            class_list=None, timers=None, vis_dir=None, vis=False, img_fmt='jpg', angle=None,
            class_id=None):

    # use this function only to refine the detections to use as annos
    # TODO: How to format proposals variable for multi-claass??
    dets_proposals = copy.deepcopy(detPB[:, 2:6])
    dets_proposals[:, 2:4] = dets_proposals[:, 0:2] + dets_proposals[:, 2:4]
    cls_boxes, cls_segms = regressed_box2mask(maskRCNN, im,
                                    box_proposals=dets_proposals,
                                    timers=timers,
                                    test_aug=0,
                                    soft_nms=0,
                                    nms_thr=0.7,
                                    score_thr=0)
    det_size = sum(len(cls_boxes[cl]) for cl in class_list)
    assert len(cls_boxes[class_id])==len(detPB)
    #assert len(class_list)==1, 'regression for augmented dets only workd for single class model.. mulit-class: full input proposals are used for all classes'
    assert det_size//len(class_list)==dets_proposals.shape[0], 'proposals size {}, prediction size {}'.format(det_size, dets_proposals)
    if fr % 1 == 0 and vis and det_size > 0:
        vis_img = vis_utils.vis_clasp_annos(
                fr,
                angle,
                vis_img,  # BGR -> RGB for visualization
                im_name,
                vis_dir,
                cls_boxes,
                segms=cls_segms,
                dataset=dataset,
                class_list=[class_id],
                box_alpha=1,
                show_class=True,
                thresh=pred_score,
                kp_thresh=2,
                ext=img_fmt,
                show_mask=1
            )

    if det_size > 0:
        det_class, mask_class = get_box_mask(fr, cls_boxes, cls_segms, thresh=0,
                                    angle=angle, class_list=[class_id], dataset=dataset)
    else:
        mask_class = []
    det_class = np.array(det_class)
    assert  len(det_class)==len(mask_class)==len(detPB)
    return det_class, mask_class, vis_img

def rotated_boxes(masks, img_rot=None, angle=None):
    #rle to binary mask
    # rotated boxes are axis aligned
    rot_boxes = []
    for mask in masks:
        mask_org = mask_util.decode(mask)
        # binary_mask = np.array(mask_image>0.5, dtype=np.uint8) # convert scoremap into binary mask
        # mask from segm_results
        mask_org[np.where(mask_org > 0)] = 255

        mask_rot = imutils.rotate_bound(mask_org, angle)  # mask_image scaled to rotated image size
        assert mask_rot.shape == img_rot.shape[0:2]
        # assert img_org.shape==im.shape[0:2], 'remapped mask image: {}, original image: {}'.format(img_org.shape, im.shape[0:2])
        rot_boxes.append(np.array(cv2.boundingRect(mask_rot)))
    return np.array(rot_boxes)

def get_split(GT, train_split=0.8, cam=None):
    """

    :param GT:
    :param train_splt:
    :param cam:
    :return:
    """
    GTp = [gt for gt in GT if gt[9] == 1]
    GTb = [gt for gt in GT if gt[9] == 2]
    gt_frameset = np.unique(GT[:, 0].astype('int'))
    gt_len = len(gt_frameset)
    print('full set {}: Nfr {}, person {} bag {}'.format(cam, gt_len, len(GTp), len(GTb)))

    # random split: keep split similar for training and testing forever
    random.seed(42)
    train_subset = random.sample(list(gt_frameset), int(gt_len * train_split))
    print('random sample {}'.format(train_subset[2]))
    # print(subset)
    train_GTp = [gt for gt in GT if gt[0] in train_subset and gt[9] == 1]
    train_GTb = [gt for gt in GT if gt[0] in train_subset and gt[9] == 2]
    print('train split {}: Nfr {}, person {} bag {}'.format(cam, len(train_subset), len(train_GTp), len(train_GTb)))

    test_subset = np.array([t for t in gt_frameset if t not in train_subset])
    test_GTp = [gt for gt in GT if gt[0] not in train_subset and gt[9] == 1]
    test_GTb = [gt for gt in GT if gt[0] not in train_subset and gt[9] == 2]
    print(
        'test split {}: Nfr {}, person {} bag {}'.format(cam, len(test_subset), len(test_GTp), len(test_GTb)))
    print('-------------------------------------------------')
    return train_subset, test_subset, train_GTp, train_GTb

def get_annos_cluster_mode(maskRCNN, cluster_modes, masks_0 = None, imgrot=None, class_list=None,
                        im_name=None, fr_num=None, fr=None, pred_score=0, dataset=None,
                        vis_dir=None, img_fmt='jpg', angle=None, vis_annos=False, verbose=False):
    if vis_annos:
        imgrot_vis, fig = init_fig(copy.deepcopy(imgrot))
    else:
        imgrot_vis = fig = None

    if angle == 0:
        det_cl = {}
        mask_cl = {}
        for cl in class_list:
            det_cl[cl] = cluster_modes[cluster_modes[:, 7] == cl]
            if len(det_cl[cl]) > 0:
                det_cl[cl], mask_cl[cl], imgrot_vis = get_masks(maskRCNN, det_cl[cl], im=imgrot, vis_img=imgrot_vis,
                                                                im_name=os.path.basename(im_name),
                                                                fr=fr_num, pred_score=0, dataset=dataset,
                                                                class_list=class_list,
                                                                vis_dir=vis_dir, vis=vis_annos, img_fmt='jpg',
                                                                angle=angle, class_id=cl)
        box_list = [b for _, b in det_cl.items() if len(b) > 0]
        mask_list = [m for _, m in mask_cl.items() if len(m) > 0]
        if len(box_list)>0:
            fr_det = np.concatenate(box_list)
            fr_mask = np.concatenate(mask_list)
        else:
            # return empty pseudo labels
            fr_det = fr_mask = cluster_modes = []

            if vis_annos:
                fig.savefig(os.path.join(vis_dir, '{}_{}'.format(angle, os.path.basename(im_name))), dpi=200)
                plt.close('all')

        assert len(fr_det) == len(fr_mask) == len(cluster_modes)
        masks_0 = copy.deepcopy(fr_mask)

        #print('collect training examples: frame: {}, #detection: {}, angle: {}'.format(fr,
                                                                                       #len(fr_det),
                                                                                       #angle))
    else:
        # remap cluster modes from theta=0 to theta= 6, 84, 90 ,....
        assert masks_0 is not None
        rotated_modes = rotated_boxes(masks_0, img_rot=imgrot, angle=angle)
        cluster_modes[:, 2:6] = rotated_modes

        det_cl = {}
        mask_cl = {}
        for cl in class_list:
            det_cl[cl] = cluster_modes[cluster_modes[:, 7] == cl]
            if len(det_cl[cl]) > 0:
                det_cl[cl], mask_cl[cl], imgrot_vis = get_masks(maskRCNN, det_cl[cl], im=imgrot, vis_img=imgrot_vis,
                                                                im_name=os.path.basename(im_name),
                                                                fr=fr_num, pred_score=0, dataset=dataset,
                                                                class_list=class_list,
                                                                vis_dir=vis_dir, vis=vis_annos, img_fmt='jpg',
                                                                angle=angle, class_id=cl)
        box_list = [b for _, b in det_cl.items() if len(b) > 0]
        mask_list = [m for _, m in mask_cl.items() if len(m) > 0]
        if len(box_list)>0:
            fr_det = np.concatenate(box_list)
            fr_mask = np.concatenate(mask_list)
        else:
            # return empty pseudo labels
            fr_det = fr_mask = cluster_modes = []

            if vis_annos:
                fig.savefig(os.path.join(vis_dir, '{}_{}'.format(angle, os.path.basename(im_name))), dpi=200)
                plt.close('all')

        assert len(fr_det) == len(fr_mask) == len(cluster_modes)
        if verbose:
            print('collect training examples: frame: {}, #detection: {}, angle: {}'.format(fr,
                                                                                           len(fr_det),
                                                                                           angle))
    return  fr_det, fr_mask, masks_0


def gtbox2modes(anns_fr, fr_num, angle=0):
    assert len(anns_fr) > 0
    fr_det = []
    for i, ann in enumerate(anns_fr):
        box = ann['bbox']
        score = 1
        catId = ann['category_id']
        fr_det.append([fr_num, i, box[0], box[1], box[2], box[3], score, catId, angle])
    return np.array(fr_det)

def get_random_angles(ranges=None, factor=None):
    angleSet = [0]
    for intvl in ranges:
        angleSet += random.sample(range(intvl[0], intvl[1]), factor)
    return angleSet

def main(all_frames, semi_frames, percent_clasp_gt, angleSet, dataset_clasp, input_imgs_dir,
         output_dir, isTrain, imgResultDir, soft_nms=False,
         class_list=None, data_type='gt', save_data=False,
         saveAugResult=False, apply_cluster_mode=False,
        det_thr=0.5, nms_thr=0.5, all_scores=None, regress_cluster=None, vis_annos=False,
         cluster_score_cams=None, ranges=None, database='clasp1', gpu_id=1, verbose=False):
    """main function"""
    # configure detector
    if not torch.cuda.is_available():
        sys.exit("Need a CUDA device to run the code.")
    torch.cuda.set_device(gpu_id)
    torch.set_num_threads(1)
    #args = parse_args()
    print('Called with args:')
    #print(args)

    if data == 'coco':
        dataset = datasets.get_coco_dataset()
        cfg.MODEL.NUM_CLASSES = len(dataset.classes)
    elif data == 'clasp2020':
        dataset = datasets.get_clasp_dataset()
        cfg.MODEL.NUM_CLASSES = len(dataset.classes)
    elif data == 'clasp1_2021':
        dataset = datasets.get_clasp1_dataset()
        cfg.MODEL.NUM_CLASSES = len(dataset.classes)
    elif data == 'clasp2_2021':
        dataset = datasets.get_clasp1_dataset()
        cfg.MODEL.NUM_CLASSES = len(dataset.classes)
    elif data=="keypoints_coco":
        dataset = datasets.get_coco_dataset()
        cfg.MODEL.NUM_CLASSES = 2
    else:
        raise ValueError('Unexpected dataset name: {}'.format(data))

    print('load cfg from file: {}'.format(cfg_file))
    cfg_from_file(cfg_file)

    # set NMS
    cfg['TEST']['NMS'] = nms_thr
    cfg['TEST']['SCORE_THRESH'] = det_thr

    if set_cfgs is not None:
        cfg_from_list(set_cfgs)
    assert bool(load_ckpt) ^ bool(load_detectron), \
        'Exactly one of --load_ckpt and --load_detectron should be specified.'
    cfg.MODEL.LOAD_IMAGENET_PRETRAINED_WEIGHTS = False  # Don't need to load imagenet pretrained weights
    assert_and_infer_cfg()
    maskRCNN = Generalized_RCNN()
    if cuda:
        maskRCNN.cuda()

    load_name = load_ckpt
    print("loading checkpoint %s" % (load_name))
    checkpoint = torch.load(load_name, map_location=lambda storage, loc: storage)
    if load_ckpt_frcnn:
        checkpoint_frcnn = torch.load(load_ckpt_frcnn, map_location=lambda storage, loc: storage)
        checkpoint_frcnn = checkpoint_frcnn['model']
    else:
        checkpoint_frcnn = None
    net_utils.load_ckpt(maskRCNN, checkpoint['model'], ckpt_frcnn=checkpoint_frcnn,
                        isTrain=False, FreezeResnetConv=False)

    if load_detectron:
        print("loading detectron weights %s" % load_detectron)
        load_detectron_weight(maskRCNN, load_detectron)

    maskRCNN = mynn.DataParallel(maskRCNN, cpu_keywords=['im_info', 'roidb'],
                                 minibatch=True, device_ids=[0])  # only support single GPU
    maskRCNN.eval()


    # read imgaes from multi-camera clasp2 data folders
    #cam_list = [folders[0],folders[2],folders[4],folders[5],folders[6]]
    import cProfile
    import pstats
    pr = cProfile.Profile()

    # open text files for saving detections
    save_dets_dict = {}
    #save all images in one folder
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    # create and clean path for visualization
    result_path = os.path.join(imgResultDir, 'vis')
    if not os.path.exists(result_path):
        os.makedirs(result_path)
    else:
        delete_all(result_path, fmt='jpg')
    # to save final det from MI-MS
    save_dets_dict['dets_aug'] = open(
        os.path.join(result_path, '{}'.format('all_cam' + '_pb_1aug1nms.txt')), mode='w')
    fr = 1
    annIDcount = 1

    for fr_num in all_frames:
        im_name = os.path.join(input_imgs_dir, '{:08d}.png'.format(fr_num))
        # search training set frames for augmented detections
        #pr.enable()
        #for angle set of size 20
        angleSet = get_random_angles(ranges=ranges, factor=4)
        im = cv2.imread(im_name)

        rot_imgs_dict = {}
        detPB = []
        maskPB_rot = []
        detPB_rot = []
        masks_0 = None
        for angle in angleSet:
            if angle>0:
                if verbose:
                    print('Image: {}, Cam: {}, Rotated by: {},'.format(os.path.basename(im_name),
                                                                   im_name.split('/')[-3], angle))
                imgrot = imutils.rotate_bound(im, angle)
            else:
                imgrot = im
            rot_imgs_dict[angle] = copy.deepcopy(imgrot)

            timers = defaultdict(Timer)
            start_time = time.time()
            cls_boxes, cls_segms, cls_keyps, cls_segms_coarse = im_detect_all(maskRCNN, imgrot,
                                                                              timers=timers,
                                                                              test_aug=False,
                                                                              soft_nms=True,
                                                                              nms_thr=nms_thr,
                                                                              score_thr=det_thr)
            if verbose:
                print("Execution time {} sec".format(time.time() - start_time))
            if cls_boxes is not None:
                dets_rot, masks_rot = get_box_mask(fr_num, cls_boxes, cls_segms, cls_keyps, thresh=det_thr,
                                               angle=angle, class_list=class_list, dataset=dataset, verbose=verbose)
                #get remapped dets and maintain the index of dets_rot
                dets, _, _ = vis_utils.return_box_mask(
                    imgrot[:, :, ::-1],
                    im[:, :, ::-1],
                    fr_num,
                    angle,
                    cls_boxes,
                    cls_segms,
                    cls_segms_coarse,
                    dataset=dataset,
                    class_list=class_list,
                    save_coarse=False,
                    thresh=det_thr,
                    verbose=verbose
                    )
                raw_det_len = sum([len(cls_boxes[id]) for id in class_list])
                assert raw_det_len==len(dets) == len(masks_rot) == len(dets_rot),\
                'found #raw dets: {} #remap dets: {}, #masks_rot: {}, #dets_rot: {}'.format(
                    raw_det_len, len(dets),len(masks_rot),len(dets_rot))

                #collect augmented dets to apply MI-MS
                if (len(dets) > 0):
                    for i, box in enumerate(dets):
                        # append detections for each orientation
                        #TODO: use det score: pax:0.5, bag:0.4
                        #
                        if database in ['clasp1', 'clasp2']:
                            if (box[6] >= 0.5 and box[7] == 1) or (box[6] >= 0.5 and box[7] != 1):
                                detPB.append(box)
                                maskPB_rot.append(masks_rot[i])
                                detPB_rot.append(dets_rot[i])

                        all_scores['dets']['det_score'].append(box[6])
                        all_scores['dets']['class_id'].append(box[7])
                        all_scores['dets']['frame'].append(fr)


        #apply MS on the augmented det set to get less noisy examples for active learning
        detPB = np.array(detPB)
        detPB_rot = np.array(detPB_rot)
        assert len(detPB)==len(detPB_rot)

        if len(detPB) > 0 and saveAugResult:
            # save_coarse_mask_PANet(PAXdet,PAXmask,out_path)
            # regress bbox to reduce noise in remapped distribution
            #TODO: apply regress only when the number of class<=2
            if regress_cluster:
                # apply regression on the augmented set separately for each class
                det_cl = {}
                for cl in class_list:
                    det_cl[cl] = detPB[detPB[:, 7] == cl]
                    det_befr_reg = copy.deepcopy(det_cl[cl])
                    # regress to reduce noise in remap
                    if len(det_cl[cl]) > 0:
                        class_dets = regress_dets(maskRCNN, proposals=det_cl[cl], im=im, im_name=os.path.basename(im_name), fr=fr_num,
                                                  pred_score=0.3, isNMS=1, nms_thr=0.3, dataset=dataset,
                                                  class_list=class_list, timers=timers, output_dir=output_dir,
                                                  vis=0, img_fmt='jpg', class_id=cl)
                        det_cl[cl] = class_dets
                        # !!! is the regressed dets maintain class specific cardinality??
                        #assert len(det_befr_reg)==len(det_cl[cl]),\
                            #'proposals size {} prediction size {}'.format(len(det_befr_reg), len(det_cl[cl]))
                # detPB will be updated using regressed dets whatever the objectness score: no filtering using det_thr or nms
                #TODO: combine rotational invariance and instance uncertainty
                #TODO: detPB[:, 7] = detPB[:, 7]*regressed_dets[:, 7]
                #print('det_cl: {}'.format(det_cl))
                box_list = [b for _, b in det_cl.items() if len(b) > 0]

                if len(box_list) > 0:
                    regressed_dets = np.concatenate(box_list)
                    #assert len(detPB) == len(regressed_dets)

                    #TODO: detPB[:, 2:6] = regressed_dets[:, 2:6]
                    # <<<<do not update the prediction score of the augmented proposals after regression>>>>
                    detPB = regressed_dets
                else:
                    detPB = []

            if len(detPB)>0:
                #call cluster selection method on the regressed test-time augmented predictions
                if fr_num % 100 == 0:
                    show_result = True
                else:
                    show_result = False
                cluster_score_thr = [0.02, 0.02] #cluster_score_cams[im_name.split('/')[-3]]
                MI_MS = Cluster_Mode(detPB, fr_num, angleSet, im,
                                result_path, save_dets_dict, vis=show_result,
                                save_modes=True, cluster_scores_thr=cluster_score_thr,
                                nms=soft_nms, save_scores=all_scores, global_frame=fr,
                                     im_name=im_name, verbose=verbose)
                det_indexs, all_scores, cluster_modes = MI_MS.get_modes()
                if verbose:
                    print("Execution time with augmentation {} sec".format(time.time() - start_time))
            else:
                det_indexs = cluster_modes = []

        #cluster_modes = detPB
        #here det_indexs will be the position of the selected clusters dets in detPB
        if len(cluster_modes)>0 and save_data:
            #start loop for all unique angles and save corresponding image and detections
            #[CXbox, CYbox, fr, detBox[1], x, y, w, h, score, classID, angle]
            # use percent GT json for semi-supervised
            if semi_supervised and fr_num in semi_frames:
                anns_fr = get_frame_anns(percent_clasp_gt, fr_num)
                # fr_det = [ann['bbox'] for ann in anns_fr]
                # no mask since manual annotations only contain box
                # fr_mask = [ann['segmentation'] for ann in anns_fr]
                # update cluster modes: [fr,ind,x,y,w,h,score,class,angle] from box gt
                # and then apply augmentation
                # cluster_modes (only box) are replaced from the manual labels
                cluster_modes = gtbox2modes(anns_fr, fr_num, angle=0)

            for theta in angleSet: #[0, 6,  84, 90, 96,  174, 180, 186, 270, 354]:#angleSet:
                # TODO: use random set of rotation angles in each iteration
                #TODO: use 10 angles when the clusters modes are used as reference ann in rotated frames
                #TODO: ISSUES: Sometimes cluster modes can not create separate detections for visually merged pax
                #TODO: use all angles and corresponding predictions belongs to a single cluster: recover cluster mode failures

                imgrot = rot_imgs_dict[theta]

                if apply_cluster_mode:
                    # TODO: apply cluster modes for rotated pseudo-labels from second iteration
                    #regress cluster mode: score is updated after regression
                    if verbose:
                        print('predicted cluster mode score: {}'.format(cluster_modes[:,6]))
                    fr_det,  fr_mask, masks_0 = get_annos_cluster_mode(maskRCNN, cluster_modes, masks_0,  imgrot=imgrot, class_list=class_list,
                                                             im_name=im_name, fr_num=fr_num, pred_score=0, dataset=dataset, vis_annos=False,
                                                         vis_dir=None, img_fmt='jpg', angle=theta, verbose=verbose)
                else:
                    fr_det = []
                if len(fr_det)>0:
                    if verbose:
                        print('training examples: frame: {}, #detection: {}, angle: {}'.format(fr,
                                                                                                   len(fr_det),
                                                                                                   theta))
                    #save image info
                    imgrot = rot_imgs_dict[theta]
                    imgIdnew = 10000 * int('%06d' % fr) + theta
                    imgname = '{:08d}.png'.format(imgIdnew)
                    img_write_path = output_dir + '/' + imgname

                    if not os.path.exists(img_write_path):
                        dataset_clasp = Write_ImagesInfo(imgrot, imgname, int(imgIdnew), dataset_clasp)
                        if verbose:
                            print('Writing image {}'.format(imgname))
                        cv2.imwrite(img_write_path, imgrot)

                    dataset_clasp = Write_ImagesInfo(imgrot, imgname, int(imgIdnew), dataset_clasp)

                    # save det info
                    if verbose:
                        print('instance certainty score: {}, rotation invariant score: {}'.format(
                            fr_det[:, 6],
                            cluster_modes[:, 6]))
                    #instance uncertainties are replaced with rotation invariance
                    #fr_det[:, 6] = cluster_modes[:, 6]
                    # When we use nms on the augmented proposals, the instance score is used instead of cluster score

                    for ib,box in enumerate(fr_det):
                        # no score threshold or cluster score instead of detection score:
                        # since only consider the cluster modes
                        if (box[6]>=cluster_score_thr[0] and box[7]==1) or \
                            (box[6]>=cluster_score_thr[1] and box[7]!=1):

                            bboxfinal = [round(x, 2) for x in box[2:6]]
                            mask = fr_mask[ib]
                            area = mask_util.area(mask)#polygon area
                            assert len(box)==9, 'box {}'.format(box)
                            #[fr, i, bbox[0], bbox[1], w, h, score, classes[i]]
                            if box[7]==1:
                                catID = 1
                            else:
                                catID = 2
                            #score: cluster_score or cluster_mode regressed score
                            score = box[6]
                            #for SSL only
                            #score = -1

                            annID = 1000 * int('%06d' % (annIDcount)) + theta
                            annIDcount+=1
                            #box = mask_util.toBbox(mask)
                            # convert rle mask into polygon mask
                            #TODO: try to use rle format in coco annotation format
                            segmPolys = []  # mask['counts'].decode("utf-8") #[]

                            bmask = mask_util.decode(mask)
                            contours = measure.find_contours(bmask, 0.5)
                            #contours = cv2.findContours(bmask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

                            for contour in contours:
                                contour = np.flip(contour, axis=1)
                                segmentation = contour.ravel().tolist()
                                if len(segmentation)>0:
                                    segmPolys.append(segmentation)
                            assert int(imgIdnew)== int(os.path.basename(img_write_path).split('.')[0])
                            #save annotation for for each image
                            if len(segmPolys)>0:
                                dataset_clasp = Write_AnnotationInfo(bboxfinal, segmPolys, int(imgIdnew),
                                                                    int(annID), catID, int(area), dataset_clasp,
                                                                             instance_certainty=score)

                    fr+=1

                #pr.disable()
                #sortby = 'cumulative'
                #print('Frame: {} pseudo-labels profile..............................'.format(fr-1))
                #ps = pstats.Stats(pr, stream=sys.stdout).sort_stats(sortby).print_stats(20)

    return dataset_clasp, all_scores

def delete_all(demo_path, fmt='png'):
    filelist = glob.glob(os.path.join(demo_path, '*.' + fmt))
    if len(filelist) > 0:
        for f in filelist:
            os.remove(f)

if __name__ == '__main__':
    storage = '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61'
    isTrain = False
    database = 'clasp2'
    cam_type = 'pri'
    data_type = 'clasp2_augMS'  #'clasp1_augMS' #'clasp2_augMS'  # 'gt' #'test_aug
    # required inputs
    data ='clasp2_2021'# 'coco'#'clasp1_2021' #'coco'#  # 'clasp1_2021' #'clasp1_2021' #'clasp1_2021' #'coco' #'clasp2020' #
    #model_path = '/home/siddique/PANet/Outputs/e2e_panet_R-50-FPN_2x_mask/'
    model_path = '/media/siddique/464a1d5c-f3c4-46f5-9dbb-bf729e5df6d61/PANet_Models/'
    cfg_file = '/home/siddique/PANet/configs/panet/e2e_panet_R-50-FPN_2x_mask.yaml'

    load_ckpt_frcnn = None  # model_path+'e2e_panet_R-50-FPN_1x_det/panet_box_tuned/ckpt/model_step25524.pth'
    images = False
    set_cfgs = None
    load_detectron = None
    cuda = True
    merge_pdfs = False

    verbose = True
    gpu_id=0
    vis_annos = 0
    semi_supervised = 1
    percent_gt = 10

    regress_cluster=1 # 0: skip regress_cluster()
    apply_cluster_mode = 1

    alpha_loss=1
    alpha_loss_sigm = 0

    save_data = 1
    test_aug = 1
    soft_nms = 1
    nms_thr = 0.3
    exp =4 #8-11 for 2, 5 models
    if exp==0:
        data = 'coco'
    # recommended: pax: 0.4, bag: 0.2
    # higher cluster score decrease the possibility of negative examples from the augmented dets...
    #10:[0.8, 0.8], 9:[0.75, 0.75], 8:[0.6, 0.6], 7:[0.65, 0.7],6:[0.6, 0.6], 4-5:[0.5, 0.65]#3:[0.55, 0.5] #2:[0.5, 0.35]#clasp2: 0-1:[0.4, 0.25]
    if database=='clasp2':

        if cam_type=='pri':
            # w reg : 0.1, wo_reg:0.2
            pb_score = [0.1, 0.1]
            cluster_score_cams = {'G_9': pb_score, 'G_11': pb_score,
                                  'H_9': pb_score, 'H_11': pb_score,
                                  'I_9': pb_score, 'I_11': pb_score}
        if cam_type=='aux':
            cluster_score_cams = {'G_2': [0.1, 0.1], 'G_5': [0.1, 0.1],
                                  'H_2': [0.1, 0.1], 'H_5': [0.1, 0.1],
                                  'I_2': [0.1, 0.1], 'I_5': [0.1, 0.1]}
    else:
        #pred_score: {0:[0.5,0.5], 1:[0.7, 0.5], 1:[0.8, 0.5]}
        #cluster_score: {0:[0.1,0.1], 1:[0.3,0.1], 2:[0.6,0.1]}
        p_score = 0.4 #alpha_loss:0.2, wo_alpha: 0.4
        b_score = 0.2
        cluster_score_cams = {'A_9': [p_score, b_score], 'A_11': [p_score, b_score],
                              'B_9': [p_score, b_score], 'B_11': [p_score, b_score],
                              'C_9': [p_score, b_score], 'C_11': [p_score, b_score],
                              'D_9': [p_score, b_score], 'D_11': [p_score, b_score],
                              'E_9': [p_score, b_score], 'E_11': [p_score, b_score]
                          }
    det_thr = 0.7 #clasp2:0.5-0.7, clasp1:0.5
    #model selection
    if data == 'coco':
        load_ckpt = '/home/siddique/PANet/panet_mask_step179999.pth'
        class_list = [1, 27, 25, 29]
    else:
        if data_type=='clasp1_augMS':
            if alpha_loss:
                #load_ckpt = model_path + 'clasp1/modified_loss/iter{}/ckpt/model_step19999.pth'.format(exp - 1)
                load_ckpt = model_path + 'clasp1/modified_loss_wo_reg/iter{}/ckpt/model_step19999.pth'.format(exp - 1)
            elif alpha_loss_sigm:
                load_ckpt = model_path + 'clasp1/loss_sigmoid/iter{}/ckpt/model_step19999.pth'.format(exp - 1)
            else:
                load_ckpt = model_path + 'clasp1/wo_score_regress_loss/iter{}/ckpt/model_step19999.pth'.format(exp-1)
        else:
            if alpha_loss:
                load_ckpt = model_path + 'clasp2/modified_loss_semi/iter{}/ckpt/model_step19999.pth'.format(exp - 1)
                #load_ckpt = model_path + 'clasp2/modified_loss_wo_reg/iter{}/ckpt/model_step19999.pth'.format(exp - 1)
            elif alpha_loss_sigm:
                load_ckpt = model_path + 'clasp2/loss_sigmoid/iter{}/ckpt/model_step19999.pth'.format(exp - 1)
            else:
                load_ckpt = model_path + 'clasp2/wo_score_regress_loss/iter{}/ckpt/model_step19999.pth'.format(exp-1)

        print('load model {} for iteration {}'.format(load_ckpt, exp))
        # 'e2e_panet_R-50-FPN_2x_mask/panet_box_mask_tuned3k/ckpt/model_step9999.pth'
        class_list = [1, 2]

    if data_type=='clasp2_augMS':
        benchmark = storage + '/tracking_wo_bnw/data/CLASP/'
        benchmark_path = storage + '/tracking_wo_bnw/data/CLASP/train_gt_all/PB_gt/'
        if alpha_loss:
            imgResultDir = benchmark + 'test_augMS_gt_score/iter{}'.format(exp)
            #imgResultDir = benchmark + 'test_augMS_gt_score_wo_regress/iter{}'.format(exp)

        elif alpha_loss_sigm:
            imgResultDir = benchmark + 'test_augMS_gt_score_sigm/iter{}'.format(exp)
        else:
            imgResultDir = benchmark + 'test_augMS_gt_wo_regress/iter{}'.format(exp)

        savefilename = imgResultDir + '/clasp2_test_aug_{}.json'.format(exp)
        SaveImgDir = imgResultDir + '/img1_{}'.format(exp)
        #to compute the det score and cluster score threshold
        all_scores = {'dets':{'det_score':[], 'class_id':[], 'frame':[]},
                      'clusters':{'cluster_score':[], 'class_id':[], 'frame':[]}}

        det_score_file = imgResultDir + '/det_scores_clasp2_test_aug_{}.csv'.format(exp)
        cluster_score_file = imgResultDir + '/cluster_scores_clasp2_test_aug_{}.csv'.format(exp)

    if data_type == 'clasp1_augMS':
        benchmark = storage + '/tracking_wo_bnw/data/CLASP1/'
        benchmark_path = storage + '/tracking_wo_bnw/data/CLASP1/train_gt/'
        if alpha_loss:
            #imgResultDir = benchmark + 'test_augMS_gt_score/iter{}'.format(exp)
            imgResultDir = benchmark + 'test_augMS_gt_score_wo_regress/iter{}'.format(exp)

        elif alpha_loss_sigm:
            imgResultDir = benchmark + 'test_augMS_gt_score_sigm/iter{}'.format(exp)
        else:
            imgResultDir = benchmark + 'test_augMS_gt_wo_regress/iter{}'.format(exp)

        savefilename = imgResultDir + '/clasp1_test_aug_{}.json'.format(exp)
        SaveImgDir = imgResultDir + '/img1_{}'.format(exp)
        #to compute the det score and cluster score threshold
        all_scores = {'dets':{'det_score':[], 'class_id':[], 'frame':[]},
                      'clusters':{'cluster_score':[], 'class_id':[], 'frame':[]}}

        det_score_file = imgResultDir + '/det_scores_clasp1_test_aug_{}.csv'.format(exp)
        cluster_score_file = imgResultDir + '/cluster_scores_clasp1_test_aug_{}.csv'.format(exp)

    # If semi-supervised
    if semi_supervised:
        #load gt json for percent gt data: used as manual annotations
        json_path = storage + '/SoftTeacher/data/{}/annotations/semi_supervised/instances_train_{}.1@{}.json'.format(database, database, percent_gt)
        percent_clasp_gt, semi_frames = load_clasp_json(json_path, percent=percent_gt)

        #load full gt data
        json_path = storage + '/SoftTeacher/data/{}/annotations/instances_train_{}.json'.format(database, database)
        _, all_frames = load_clasp_json(json_path, percent=100)

    if test_aug:
        #TODO: select random angles from some specific ranges
        angleSet = [0, 6, 12, 78, 84, 90, 96, 102, 168, 174, 180, 186, 192, 258, 264, 270, 276, 342, 348, 354]
        saveAugResult=1
        ranges = [[1, 24], [78, 96], [168, 192], [258, 276], [342, 354]]
    else:
        angleSet = [0]
        ranges=None

    dataset_clasp = define_dataset_dictionary()

    input_imgs_dir = storage + '/SoftTeacher/data/{}/train{}'.format(database, database.upper())
    dataset_clasp, all_scores = main(all_frames, semi_frames, percent_clasp_gt, angleSet, dataset_clasp, input_imgs_dir,
                                     SaveImgDir, isTrain, imgResultDir,
                                     soft_nms, class_list, data_type, save_data, saveAugResult, apply_cluster_mode,
                                     det_thr, nms_thr, all_scores, regress_cluster, vis_annos, cluster_score_cams,
                                     ranges, database, gpu_id=gpu_id, verbose=verbose)
    if save_data:
        Write_To_Json(savefilename, dataset_clasp)

        Dframe_dets = pd.DataFrame(all_scores['dets'])
        Dframe_dets.to_csv(det_score_file, mode='w', index=False)

        Dframe_clusters = pd.DataFrame(all_scores['clusters'])
        Dframe_clusters.to_csv(cluster_score_file, mode='w', index=False)
